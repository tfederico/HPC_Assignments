{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "<br/>\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# Spark Structured API: DataFrames and SQL\n",
    "In the previous notebook you have seen distributed processing using RDDs is done. In this notebook we will look at Spark's Structured API. We will see how you can use DataFrames and SQL to do common data processing operations. By the end you should have a feeling for the strengths and weaknesses of these different approaches.\n",
    "\n",
    "**Please complete the assignments in this notebook, and download your notebook: 'File' -> 'Download' -> 'Notebook (.ipynb)'. Send this notebook to [`helpdesk@surfsara.nl`](mailto:helpdesk@surfsara.nl), with your name. Please mention 'UvA HPC course' in your email subject.**\n",
    "\n",
    "The first difference is our Spark _entrypoint_. For RDDs this was the 'SparkContext' (usually named `sc`). For DataFrames we will use a 'SparkSession', which is more powerful and easier to use. By convention we name our SparkSession `spark`, and we create it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use a SparkSession to create DataFrames (as we will soon see) and these can be converted to RDDs. However if we directly want to create RDDs we have to do this via SparkContext. A SparkContext is contained in SparkSession, and can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize(['a', 'b', 'c'])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DataFrames from Python collections\n",
    "\n",
    "Just like we have seen with `sc.parallelize` for RDDs, we can create a DataFrame from an existing Python collection. In addition to the collection itself we will also describe (part of) the structure of the data by naming the columns. Additionally, we could  specify the data types of the columns, but in this case we can let Spark infer this automatically.\n",
    "\n",
    "First, a list of tuples in Python is created, called `phone_stock`. Next, we create a list called `columns` that contain the name of all columns of the DataFrame. Then we use these two lists as input for [`createDataFrame`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame). The result is the DataFrame `phone_df`. Next we print the type of both `phone_stock` and `phone_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of phoneStock: <class 'list'>\n",
      "the type of phone_df: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "phone_stock = [\n",
    "    ('iPhone 6', 'Apple', 6, 549.00),\n",
    "    ('iPhone 6s', 'Apple', 5, 585.00),\n",
    "    ('iPhone 7', 'Apple', 11, 739.00),\n",
    "    ('Pixel', 'Google', 8, 859.00),\n",
    "    ('Pixel XL', 'Google', 2, 959.00),\n",
    "    ('Galaxy S7', 'Samsung', 10, 539.00),\n",
    "    ('Galaxy S6', 'Samsung', 5, 414.00),\n",
    "    ('Galaxy A5', 'Samsung', 7, 297.00),\n",
    "    ('Galaxy Note 7', 'Samsung', 0, 841.00)\n",
    "]\n",
    "\n",
    "columns = ['model', 'brand', 'stock', 'unit_price']\n",
    "\n",
    "phone_df = spark.createDataFrame(phone_stock, columns)\n",
    "\n",
    "print('the type of phoneStock: ' + str(type(phone_stock)))\n",
    "print('the type of phone_df: ' + str(type(phone_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to see a few rows of a DataFrame use [`show()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show). By default it shows 20 rows, but you can give the desired number of rows that you want to see as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+----------+\n",
      "|        model|  brand|stock|unit_price|\n",
      "+-------------+-------+-----+----------+\n",
      "|     iPhone 6|  Apple|    6|     549.0|\n",
      "|    iPhone 6s|  Apple|    5|     585.0|\n",
      "|     iPhone 7|  Apple|   11|     739.0|\n",
      "|        Pixel| Google|    8|     859.0|\n",
      "|     Pixel XL| Google|    2|     959.0|\n",
      "|    Galaxy S7|Samsung|   10|     539.0|\n",
      "|    Galaxy S6|Samsung|    5|     414.0|\n",
      "|    Galaxy A5|Samsung|    7|     297.0|\n",
      "|Galaxy Note 7|Samsung|    0|     841.0|\n",
      "+-------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Like RDDs we have a [`collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) action that returns all data from a DataFrame to the driver. Notice that we get `Row` objects that contain column name and value pairs. Remember that the result of a `collect()` is a Python data structure (a list of `Row` objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(model='iPhone 6', brand='Apple', stock=6, unit_price=549.0),\n",
       " Row(model='iPhone 6s', brand='Apple', stock=5, unit_price=585.0),\n",
       " Row(model='iPhone 7', brand='Apple', stock=11, unit_price=739.0),\n",
       " Row(model='Pixel', brand='Google', stock=8, unit_price=859.0),\n",
       " Row(model='Pixel XL', brand='Google', stock=2, unit_price=959.0),\n",
       " Row(model='Galaxy S7', brand='Samsung', stock=10, unit_price=539.0),\n",
       " Row(model='Galaxy S6', brand='Samsung', stock=5, unit_price=414.0),\n",
       " Row(model='Galaxy A5', brand='Samsung', stock=7, unit_price=297.0),\n",
       " Row(model='Galaxy Note 7', brand='Samsung', stock=0, unit_price=841.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phones = phone_df.collect()\n",
    "all_phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Working directly with a list of row objects is cumbersome. To work directly with data on the driver's side, we usually convert the Spark DataFrame to a `pandas` DataFrame. [`pandas`](https://pandas.pydata.org/) is a data processing library that allows us to manipulate tabular table. It is suitable for processing that isn't too intensive and data that isn't too large to fit into local memory (otherwise, why would we want to use Spark?).\n",
    "\n",
    "Spark DataFrames have a [`toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) action defined on them, that will pull all data to the driver and convert it to a `pandas` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>stock</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone 6</td>\n",
       "      <td>Apple</td>\n",
       "      <td>6</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 6s</td>\n",
       "      <td>Apple</td>\n",
       "      <td>5</td>\n",
       "      <td>585.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPhone 7</td>\n",
       "      <td>Apple</td>\n",
       "      <td>11</td>\n",
       "      <td>739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pixel</td>\n",
       "      <td>Google</td>\n",
       "      <td>8</td>\n",
       "      <td>859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pixel XL</td>\n",
       "      <td>Google</td>\n",
       "      <td>2</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Galaxy S7</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>10</td>\n",
       "      <td>539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Galaxy S6</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>5</td>\n",
       "      <td>414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Galaxy A5</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>7</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Galaxy Note 7</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>0</td>\n",
       "      <td>841.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model    brand  stock  unit_price\n",
       "0       iPhone 6    Apple      6       549.0\n",
       "1      iPhone 6s    Apple      5       585.0\n",
       "2       iPhone 7    Apple     11       739.0\n",
       "3          Pixel   Google      8       859.0\n",
       "4       Pixel XL   Google      2       959.0\n",
       "5      Galaxy S7  Samsung     10       539.0\n",
       "6      Galaxy S6  Samsung      5       414.0\n",
       "7      Galaxy A5  Samsung      7       297.0\n",
       "8  Galaxy Note 7  Samsung      0       841.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are several ways to look at the structure of a DataFrame: `printSchema`, `schema` and `describe`. [`printSchema`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema) is especially useful with complicated nested structures, because it provides a human-readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- model: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- stock: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that all columns are listed, together with their type and a boolean value that indicates whether the value for that column can be NULL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Schema's can also be listed programmatically. By calling [`schema`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.schema) we get to see the structure of the DataFrame in Sparks types. It is possible to define a schema in code by making use of these types, although we won't do this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(model,StringType,true),StructField(brand,StringType,true),StructField(stock,LongType,true),StructField(unit_price,DoubleType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is also possible to look more closely on the structure of fields, in which the columns are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(model,StringType,true),\n",
       " StructField(brand,StringType,true),\n",
       " StructField(stock,LongType,true),\n",
       " StructField(unit_price,DoubleType,true)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.schema.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[`describe`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) will compute summary statistics for numeric and string columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+------------------+------------------+\n",
      "|summary|    model|  brand|             stock|        unit_price|\n",
      "+-------+---------+-------+------------------+------------------+\n",
      "|  count|        9|      9|                 9|                 9|\n",
      "|   mean|     null|   null|               6.0| 642.4444444444445|\n",
      "| stddev|     null|   null|3.5355339059327378|220.82295573100586|\n",
      "|    min|Galaxy A5|  Apple|                 0|             297.0|\n",
      "|    max| iPhone 7|Samsung|                11|             959.0|\n",
      "+-------+---------+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data extraction\n",
    "\n",
    "Now that we have our data in a DataFrame, we want to use it to manipulate the data. Let's start by selecting subsets of the data: specific columns and/or rows.\n",
    "\n",
    "### Selecting columns\n",
    "\n",
    "Often we are not interested in all the columns of our data. DataFrames make it very easy to select only a subset by using the [`select`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) method. Realise that we are not modifying the original DataFrame, but creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        model|\n",
      "+-------------+\n",
      "|     iPhone 6|\n",
      "|    iPhone 6s|\n",
      "|     iPhone 7|\n",
      "|        Pixel|\n",
      "|     Pixel XL|\n",
      "|    Galaxy S7|\n",
      "|    Galaxy S6|\n",
      "|    Galaxy A5|\n",
      "|Galaxy Note 7|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the model column\n",
    "model_df = phone_df.select(\"model\")\n",
    "model_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also rename a column by using [`expr`](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.expr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|  brand|      mymodel|\n",
      "+-------+-------------+\n",
      "|  Apple|     iPhone 6|\n",
      "|  Apple|    iPhone 6s|\n",
      "|  Apple|     iPhone 7|\n",
      "| Google|        Pixel|\n",
      "| Google|     Pixel XL|\n",
      "|Samsung|    Galaxy S7|\n",
      "|Samsung|    Galaxy S6|\n",
      "|Samsung|    Galaxy A5|\n",
      "|Samsung|Galaxy Note 7|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "mymodel_df = phone_df.select(\"brand\", expr(\"model as mymodel\"))\n",
    "mymodel_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|  brand|        model|\n",
      "+-------+-------------+\n",
      "|  Apple|     iPhone 6|\n",
      "|  Apple|    iPhone 6s|\n",
      "|  Apple|     iPhone 7|\n",
      "| Google|        Pixel|\n",
      "| Google|     Pixel XL|\n",
      "|Samsung|    Galaxy S7|\n",
      "|Samsung|    Galaxy S6|\n",
      "|Samsung|    Galaxy A5|\n",
      "|Samsung|Galaxy Note 7|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select both the brand and model columns\n",
    "bm_df = phone_df.select('brand', 'model')\n",
    "bm_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 1\n",
    "Select the `model` and `stock` columns from `phone_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|        model|stock|\n",
      "+-------------+-----+\n",
      "|     iPhone 6|    6|\n",
      "|    iPhone 6s|    5|\n",
      "|     iPhone 7|   11|\n",
      "|        Pixel|    8|\n",
      "|     Pixel XL|    2|\n",
      "|    Galaxy S7|   10|\n",
      "|    Galaxy S6|    5|\n",
      "|    Galaxy A5|    7|\n",
      "|Galaxy Note 7|    0|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Select the model and stock columns\n",
    "ms_df = phone_df.select('model','stock')\n",
    "ms_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Filtering rows\n",
    "\n",
    "We can filter specific rows by using the DataFrame [`filter`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter) method. Please note that the [`where`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where) method is an alias for `filter`. The column specifications are the same as with the select method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select rows with phones from Google\n",
    "google_df = phone_df.filter(phone_df['brand'] == 'Google')\n",
    "\n",
    "google_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 2\n",
    "Select the rows with `unit_price` less than 550.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+----------+\n",
      "|    model|  brand|stock|unit_price|\n",
      "+---------+-------+-----+----------+\n",
      "| iPhone 6|  Apple|    6|     549.0|\n",
      "|Galaxy S7|Samsung|   10|     539.0|\n",
      "|Galaxy S6|Samsung|    5|     414.0|\n",
      "|Galaxy A5|Samsung|    7|     297.0|\n",
      "+---------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "cheap_df = phone_df.filter(phone_df['unit_price'] < 550.00)\n",
    "cheap_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Multiple filter conditions can be specified using Python's [boolean operations](https://docs.python.org/3/library/stdtypes.html#boolean-operations-and-or-not):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+----------+\n",
      "|    model| brand|stock|unit_price|\n",
      "+---------+------+-----+----------+\n",
      "| iPhone 6| Apple|    6|     549.0|\n",
      "|iPhone 6s| Apple|    5|     585.0|\n",
      "| iPhone 7| Apple|   11|     739.0|\n",
      "|    Pixel|Google|    8|     859.0|\n",
      "| Pixel XL|Google|    2|     959.0|\n",
      "+---------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.filter((phone_df.brand == 'Apple') | (phone_df.brand == 'Google')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Ordering rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use the [`orderBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) method to sort data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+----------+\n",
      "|        model|  brand|stock|unit_price|\n",
      "+-------------+-------+-----+----------+\n",
      "|    Galaxy A5|Samsung|    7|     297.0|\n",
      "|    Galaxy S6|Samsung|    5|     414.0|\n",
      "|    Galaxy S7|Samsung|   10|     539.0|\n",
      "|     iPhone 6|  Apple|    6|     549.0|\n",
      "|    iPhone 6s|  Apple|    5|     585.0|\n",
      "|     iPhone 7|  Apple|   11|     739.0|\n",
      "|Galaxy Note 7|Samsung|    0|     841.0|\n",
      "|        Pixel| Google|    8|     859.0|\n",
      "|     Pixel XL| Google|    2|     959.0|\n",
      "+-------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.orderBy('unit_price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Note: Columns specifications\n",
    "\n",
    "In the previous examples we have used various _column specifications_ for selecting and filtering data. Sometimes the more complicated ones are required because the shorter versions are ambiguous for Spark's parser. For example, all these are equivalent:\n",
    "\n",
    "```\n",
    "bm_df = phone_df.select(\"brand\", \"model\")\n",
    "bm_df = phone_df.select([\"brand\", \"model\"])\n",
    "bm_df = phone_df.select(phone_df[\"brand\"], phone_df[\"model\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the next cell we use a chain of DataFrame methods that are very similar to the SQL query language used for certain databases.\n",
    "    Notice that we use only the names of columns. Note, the use of double and single quotes in the [`where`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|    model|unit_price|\n",
      "+---------+----------+\n",
      "| iPhone 7|     739.0|\n",
      "| iPhone 6|     549.0|\n",
      "|iPhone 6s|     585.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.select(\"model\", \"unit_price\").where(\"brand='Apple'\").orderBy('stock', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "An alternative way of doing the same as the cell above is using `phone_df[\"brand\"]` in the where clause. This is longer to type but intuitively more clear and easier to read. There is no ambiguity for the Spark parser with this notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|    model|unit_price|\n",
      "+---------+----------+\n",
      "| iPhone 7|     739.0|\n",
      "| iPhone 6|     549.0|\n",
      "|iPhone 6s|     585.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.select(\"model\", \"unit_price\").where(phone_df[\"brand\"]==\"Apple\").orderBy('stock', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 3\n",
    "Select all phones with a unit price larger than 300 and of which there are more than two in stock. Display the remaining phones, ordered by brand, followed by stock. Use whatever column specification syntax you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+----------+\n",
      "|    model|  brand|stock|unit_price|\n",
      "+---------+-------+-----+----------+\n",
      "|iPhone 6s|  Apple|    5|     585.0|\n",
      "| iPhone 6|  Apple|    6|     549.0|\n",
      "| iPhone 7|  Apple|   11|     739.0|\n",
      "|    Pixel| Google|    8|     859.0|\n",
      "|Galaxy S6|Samsung|    5|     414.0|\n",
      "|Galaxy S7|Samsung|   10|     539.0|\n",
      "+---------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_df.filter((phone_df[\"unit_price\"] > 300) & (phone_df[\"stock\"]>2)).orderBy(\"brand\",\"stock\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Aggregating data\n",
    "An important part of data processing is the ability to combine multiple records, like we did with `reduceByKey`. In the DataFrame API this is a two-step process:\n",
    "\n",
    "First you group the data using the `groupBy` method. `groupBy` can operate on one or multiple columns. It will not actually perform the grouping but create a reference to a `GroupedData` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.group.GroupedData'>\n"
     ]
    }
   ],
   "source": [
    "grouped_df = phone_df.groupBy('brand')\n",
    "print(type(grouped_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After the data is grouped we can apply one of the standard aggregation functions on it. They are listed at the [GroupedData](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) API documentation. These are: `min`, `max`, `mean`, `sum` and `count`. We can apply an aggregation to all columns or to a subset of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>min(unit_price)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     brand  min(unit_price)\n",
       "0  Samsung            297.0\n",
       "1   Google            859.0\n",
       "2    Apple            549.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimum for all columns\n",
    "min_df = grouped_df.min('unit_price')\n",
    "\n",
    "min_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that the `min(unit_price)` is the name of the new column. If you want to rename a column use [`withColumnRenamed`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed). As arguments this method takes the old name and new name of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 4\n",
    "\n",
    "Compute the maximum  of the unit_price per brand and rename the resulting column to `max`.\n",
    "(We assume you can do this in one line. Feel free to adapt the cell and use more lines if you want.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung</td>\n",
       "      <td>841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>739.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     brand    max\n",
       "0  Samsung  841.0\n",
       "1   Google  959.0\n",
       "2    Apple  739.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "max_df = grouped_df.max('unit_price').withColumnRenamed('max(unit_price)','max')\n",
    "max_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we can combine different aggregations per column using the [`agg`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) method on a GroupedData instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+\n",
      "|  brand|  avg(unit_price)|sum(stock)|\n",
      "+-------+-----------------+----------+\n",
      "|Samsung|           522.75|        22|\n",
      "| Google|            909.0|        10|\n",
      "|  Apple|624.3333333333334|        22|\n",
      "+-------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take the sum of the stock column, and calculate the mean of the unit_price column, in one go\n",
    "sum_df = grouped_df.agg({'stock': 'sum', 'unit_price': 'mean'})\n",
    "\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SQL\n",
    "The SQL API aims to be ANSI-SQL SQL2003 and Hive-SQL compatible. The expressiveness is very similar to the DataFrame API. You can access the SQL API from the SparkSession by using `spark.sql`. Below is a query performed using Spark's DataFrame API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    model|\n",
      "+---------+\n",
      "| iPhone 7|\n",
      "|    Pixel|\n",
      "|Galaxy S7|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame version\n",
    "res_df = phone_df.filter(phone_df['stock'] > 7).select('model')\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The SQL version of the query requires us to 'register' the DataFrame as an SQL table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    model|\n",
      "+---------+\n",
      "| iPhone 7|\n",
      "|    Pixel|\n",
      "|Galaxy S7|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL version\n",
    "\n",
    "# Register the phone_df DataFrame within SQL as a table with name 'phones'\n",
    "phone_df.createOrReplaceTempView('phones')\n",
    "\n",
    "# Perform the SQL query on the 'phones' table\n",
    "res_df = spark.sql('SELECT model FROM phones WHERE stock > 7')\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Joining with other data sets\n",
    "Often you want to combine multiple datasets on a shared column. In this example we create an extra table with information about the phone manufacturer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+-------------+\n",
      "|company_name| hq_country|founding_year|          ceo|\n",
      "+------------+-----------+-------------+-------------+\n",
      "|      Google|        USA|         1998|Sundar Pichai|\n",
      "|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|       Apple|        USA|         1976|     Tim Cook|\n",
      "+------------+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companies = [\n",
    "    ('Google', 'USA', 1998, 'Sundar Pichai'),\n",
    "    ('Samsung', 'South Korea', 1938 ,'Oh-Hyun Kwon' ),\n",
    "    ('Apple', 'USA', 1976 ,'Tim Cook')\n",
    "]\n",
    "\n",
    "columns = ['company_name', 'hq_country', 'founding_year', 'ceo']\n",
    "\n",
    "company_df = spark.createDataFrame(companies, columns)\n",
    "company_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To join two DataFrames, we use the `join` method on one of the DataFrames. This method takes two arguments: (1) the other DataFrame, and (2) a join relation. Here we join the two DataFrames on the brand/company_name columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
      "|        model|  brand|stock|unit_price|company_name| hq_country|founding_year|          ceo|\n",
      "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
      "|    Galaxy S7|Samsung|   10|     539.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|    Galaxy S6|Samsung|    5|     414.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|    Galaxy A5|Samsung|    7|     297.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|Galaxy Note 7|Samsung|    0|     841.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
      "|        Pixel| Google|    8|     859.0|      Google|        USA|         1998|Sundar Pichai|\n",
      "|     Pixel XL| Google|    2|     959.0|      Google|        USA|         1998|Sundar Pichai|\n",
      "|     iPhone 6|  Apple|    6|     549.0|       Apple|        USA|         1976|     Tim Cook|\n",
      "|    iPhone 6s|  Apple|    5|     585.0|       Apple|        USA|         1976|     Tim Cook|\n",
      "|     iPhone 7|  Apple|   11|     739.0|       Apple|        USA|         1976|     Tim Cook|\n",
      "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = phone_df.join(company_df, phone_df['brand'] == company_df['company_name'])\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is an example of a more complicated query that combines multiple steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   model|\n",
      "+--------+\n",
      "|   Pixel|\n",
      "|iPhone 7|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All the models from USA companies with more than 7 items in stock\n",
    "result = phone_df \\\n",
    "    .join(company_df, phone_df['brand'] == company_df['company_name']) \\\n",
    "    .filter(company_df['hq_country'] == 'USA') \\\n",
    "    .filter(phone_df['stock'] > 7) \\\n",
    "    .select('model')\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 5\n",
    "\n",
    "The problem below was taken from Coursera's MOOC [Big Data Analysis with Scala and Spark](https://www.coursera.org/learn/scala-spark-big-data) by the École Polytechnique Fédérale de Lausanne. We adapted the problem for PySpark.\n",
    "\n",
    "Let's assume we have a dataset with posts from a discussion forum. The entries of the dataset consist of an authorID, the name of a subforum, the number of likes and a date. The data frame is constructed in the following cell.\n",
    "\n",
    "**We would like to know how many likes each author posted on each subforum. The table should show per subforum how many likes each author has, the highest number of likes first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from  pyspark.sql import Row\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "posts = [{'authorID' : 4, 'subforum': 'java', 'likes': 5, 'date' : 'sept 5'},\n",
    "         {'authorID' : 1, 'subforum': 'python', 'likes': 3, 'date' : 'sept 4'},\n",
    "        {'authorID' : 2, 'subforum': 'python', 'likes': 35, 'date' : 'sept 3'},\n",
    "        {'authorID' : 3, 'subforum': 'java', 'likes': 1, 'date' : 'sept 5'},\n",
    "        {'authorID' : 4, 'subforum': 'java', 'likes': 14, 'date' : 'sept 5'},\n",
    "        {'authorID' : 3, 'subforum': 'python', 'likes': 12, 'date' : 'sept 3'},\n",
    "        {'authorID' : 3, 'subforum': 'java', 'likes': 14, 'date' : 'sept 5'},\n",
    "        {'authorID' : 3, 'subforum': 'java', 'likes': 10, 'date' : 'sept 5'},\n",
    "        {'authorID' : 2, 'subforum': 'python', 'likes': 21, 'date' : 'sept 5'}]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(posts)\n",
    "df_posts = spark.createDataFrame(rdd.map(lambda x : Row(**x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Please use a [groupBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy), the [sum aggregation function](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum) and an [orderBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) to come up with the desired dataFrame. Note that you want to order in descending order.\n",
    "Also note, that you can use [`groupBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) and `orderBy` on more than one column.\n",
    "\n",
    "If you get confused, break the problem into steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+\n",
      "|authorID|subforum|# of likes|\n",
      "+--------+--------+----------+\n",
      "|       2|  python|        56|\n",
      "|       3|    java|        25|\n",
      "|       4|    java|        19|\n",
      "|       3|  python|        12|\n",
      "|       1|  python|         3|\n",
      "+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_posts.groupBy(\"authorID\",\"subforum\").sum(\"likes\").withColumnRenamed('sum(likes)','# of likes').orderBy('# of likes', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conversion to/from RDD\n",
    "\n",
    "Sometimes you want to do data manipulations which would be very easy with RDD operations, but complicated with the DataFrame API. Fortunately you can convert between DataFrames and RDDs of type 'Row'. Going from DataFrame to RDD is quite simple. Going back from RDD to DataFrame is more difficult because you need to re-apply the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apples',\n",
       " 'Apples',\n",
       " 'Apples',\n",
       " 'Googles',\n",
       " 'Googles',\n",
       " 'Samsungs',\n",
       " 'Samsungs',\n",
       " 'Samsungs',\n",
       " 'Samsungs']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_rdd = phone_df.rdd\n",
    "plural_rdd = phone_rdd.map(lambda r: r.brand + 's')\n",
    "plural_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reading structured files/sources\n",
    "One of the advantages of DataFrames is the ability to read already structured data and automatically import the structure in Spark. Spark contains readers for a number of formats such as csv, json, parquet, orc, text and jdbc. There are also third-party readers/connectors for databases such as MongoDB and Cassandra.\n",
    "\n",
    "Here we read the json-formatted tweets that we also used in the last notebook. As you can see the complicated JSON schema is inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- media: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |-- source_status_id: long (nullable = true)\n",
      " |    |    |    |-- source_status_id_str: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- symbols: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- trends: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- extended_entities: struct (nullable = true)\n",
      " |    |-- media: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |-- source_status_id: long (nullable = true)\n",
      " |    |    |    |-- source_status_id_str: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- filter_level: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- in_reply_to_status_id: long (nullable = true)\n",
      " |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |-- in_reply_to_user_id: long (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- full_name: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |-- possibly_sensitive: boolean (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- contributors: string (nullable = true)\n",
      " |    |-- coordinates: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- hashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- source_status_id: long (nullable = true)\n",
      " |    |    |    |    |-- source_status_id_str: string (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- symbols: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- trends: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- screen_name: string (nullable = true)\n",
      " |    |-- extended_entities: struct (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- source_status_id: long (nullable = true)\n",
      " |    |    |    |    |-- source_status_id_str: string (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- favorite_count: long (nullable = true)\n",
      " |    |-- favorited: boolean (nullable = true)\n",
      " |    |-- filter_level: string (nullable = true)\n",
      " |    |-- geo: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |    |-- in_reply_to_status_id: long (nullable = true)\n",
      " |    |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id: long (nullable = true)\n",
      " |    |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- place: struct (nullable = true)\n",
      " |    |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- country_code: string (nullable = true)\n",
      " |    |    |-- full_name: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- place_type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |-- possibly_sensitive: boolean (nullable = true)\n",
      " |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- retweeted: boolean (nullable = true)\n",
      " |    |-- scopes: struct (nullable = true)\n",
      " |    |    |-- followers: boolean (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- truncated: boolean (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |-- default_profile: boolean (nullable = true)\n",
      " |    |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- favourites_count: long (nullable = true)\n",
      " |    |    |-- follow_request_sent: string (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- following: string (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- is_translator: boolean (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- listed_count: long (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- notifications: string (nullable = true)\n",
      " |    |    |-- profile_background_color: string (nullable = true)\n",
      " |    |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |    |-- profile_banner_url: string (nullable = true)\n",
      " |    |    |-- profile_image_url: string (nullable = true)\n",
      " |    |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |    |-- profile_link_color: string (nullable = true)\n",
      " |    |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |    |-- profile_text_color: string (nullable = true)\n",
      " |    |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |    |    |-- time_zone: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- utc_offset: long (nullable = true)\n",
      " |    |    |-- verified: boolean (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp_ms: string (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: string (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: string (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: string (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_banner_url: string (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: long (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_df = spark.read.format(\"json\").load('../data/tweets.json')\n",
    "tweet_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This structure is squeezed into a table. When we convert to Pandas we can see what the first tweet looks like in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Wed Apr 29 13:26:48 +0000 2015</td>\n",
       "      <td>([], None, [], [], [], [(48305190, 48305190, [...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>593406077439516672</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>@OdekedeJong Omdat ik het zelf ook ervaar en m...</td>\n",
       "      <td>1430314008470</td>\n",
       "      <td>False</td>\n",
       "      <td>(False, Thu Mar 04 11:16:36 +0000 2010, False,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors coordinates                      created_at  \\\n",
       "0         None        None  Wed Apr 29 13:26:48 +0000 2015   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0  ([], None, [], [], [], [(48305190, 48305190, [...              None   \n",
       "\n",
       "   favorite_count  favorited filter_level   geo                  id  \\\n",
       "0               0      False          low  None  593406077439516672   \n",
       "\n",
       "                         ...                         place possibly_sensitive  \\\n",
       "0                        ...                          None              False   \n",
       "\n",
       "   retweet_count retweeted  retweeted_status  \\\n",
       "0              0     False              None   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "\n",
       "                                                text   timestamp_ms  \\\n",
       "0  @OdekedeJong Omdat ik het zelf ook ervaar en m...  1430314008470   \n",
       "\n",
       "   truncated                                               user  \n",
       "0      False  (False, Thu Mar 04 11:16:36 +0000 2010, False,...  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.toPandas().head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 6\n",
    "Select the name and screen_name of the user, the text field and the lang field.\n",
    "\n",
    "**Hint**: nested fields can be selected using the dot notation, i.e. `df.select('<parent>.<child>')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claudia</td>\n",
       "      <td>Claudia_NL</td>\n",
       "      <td>@OdekedeJong Omdat ik het zelf ook ervaar en m...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monique Huijdink</td>\n",
       "      <td>MiesjeB</td>\n",
       "      <td>RT @RHoogland: The game is on, vrienden. Vrijd...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Martine van Dijk</td>\n",
       "      <td>martine_vandijk</td>\n",
       "      <td>@deBeschaving Snap ik! Ben nou eenmaal wat ong...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tessebel</td>\n",
       "      <td>Tessaaatje</td>\n",
       "      <td>Jeminee ik word nu pas wakker wat is dit. Zo l...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thessa</td>\n",
       "      <td>juradoscrime</td>\n",
       "      <td>@mayravdzwaag hij was helemaal niet leuk en ik...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>De Westkrant</td>\n",
       "      <td>dewestkrant</td>\n",
       "      <td>Het lijken wel kogelgaten, maar volgens de pol...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Linda Pronk-Rijpstra</td>\n",
       "      <td>PronkRijpstra</td>\n",
       "      <td>@bvpuntcom ja hoor! Deze is van net ❤️ http://...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PostNL</td>\n",
       "      <td>PostNL</td>\n",
       "      <td>@mamarije30 Dat ga ik even voor je kijken, heb...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hilda feenstra</td>\n",
       "      <td>hildafeenstra</td>\n",
       "      <td>Na ruim 1,5 jaar vandaag mijn laatste werkdag ...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Basten</td>\n",
       "      <td>GewoonBasten</td>\n",
       "      <td>@RowfeyVFX @Jerry_Kuijper heb ik ook gezegt! i...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Maassluis Punt</td>\n",
       "      <td>MaassluisInfo</td>\n",
       "      <td>Waternetsteiger tijdelijk uit de vaart\\n\\nWoen...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Manon is Manon</td>\n",
       "      <td>manonismanon</td>\n",
       "      <td>@Cindytekent haha niet erg, ik voelde wat je b...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Eric Sweens</td>\n",
       "      <td>EricSweens</td>\n",
       "      <td>Ben jij de nieuwe Service Coördinator Elektrot...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Werken bij Amerpoort</td>\n",
       "      <td>werkenamerpoort</td>\n",
       "      <td>@Astad_EU @WeekMert @SWValkenswaard en zo niet...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Leendert Terlouw</td>\n",
       "      <td>LeendertTerlouw</td>\n",
       "      <td>RT @deBezieling: Wonderlijk: het meest nederig...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name      screen_name  \\\n",
       "0                Claudia       Claudia_NL   \n",
       "1       Monique Huijdink          MiesjeB   \n",
       "2       Martine van Dijk  martine_vandijk   \n",
       "3               Tessebel       Tessaaatje   \n",
       "4                 Thessa     juradoscrime   \n",
       "5           De Westkrant      dewestkrant   \n",
       "6   Linda Pronk-Rijpstra    PronkRijpstra   \n",
       "7                 PostNL           PostNL   \n",
       "8         hilda feenstra    hildafeenstra   \n",
       "9                 Basten     GewoonBasten   \n",
       "10        Maassluis Punt    MaassluisInfo   \n",
       "11        Manon is Manon     manonismanon   \n",
       "12           Eric Sweens       EricSweens   \n",
       "13  Werken bij Amerpoort  werkenamerpoort   \n",
       "14      Leendert Terlouw  LeendertTerlouw   \n",
       "\n",
       "                                                 text lang  \n",
       "0   @OdekedeJong Omdat ik het zelf ook ervaar en m...   nl  \n",
       "1   RT @RHoogland: The game is on, vrienden. Vrijd...   nl  \n",
       "2   @deBeschaving Snap ik! Ben nou eenmaal wat ong...   nl  \n",
       "3   Jeminee ik word nu pas wakker wat is dit. Zo l...   nl  \n",
       "4   @mayravdzwaag hij was helemaal niet leuk en ik...   nl  \n",
       "5   Het lijken wel kogelgaten, maar volgens de pol...   nl  \n",
       "6   @bvpuntcom ja hoor! Deze is van net ❤️ http://...   nl  \n",
       "7   @mamarije30 Dat ga ik even voor je kijken, heb...   nl  \n",
       "8   Na ruim 1,5 jaar vandaag mijn laatste werkdag ...   nl  \n",
       "9   @RowfeyVFX @Jerry_Kuijper heb ik ook gezegt! i...   nl  \n",
       "10  Waternetsteiger tijdelijk uit de vaart\\n\\nWoen...   nl  \n",
       "11  @Cindytekent haha niet erg, ik voelde wat je b...   nl  \n",
       "12  Ben jij de nieuwe Service Coördinator Elektrot...   nl  \n",
       "13  @Astad_EU @WeekMert @SWValkenswaard en zo niet...   nl  \n",
       "14  RT @deBezieling: Wonderlijk: het meest nederig...   nl  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_df = tweet_df.select('user.name','user.screen_name','text','lang')\n",
    "name_df.toPandas().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 7\n",
    "Count the number of tweets per user, and display the top 10 most-tweeting users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|  screen_name|count|\n",
      "+-------------+-----+\n",
      "|  GeaBijenhof|    8|\n",
      "|    Hanglip68|    7|\n",
      "|   news24hnld|    6|\n",
      "|    nuswingen|    4|\n",
      "|ItsEtienneZld|    4|\n",
      "|          KLM|    4|\n",
      "| nieuws_media|    4|\n",
      "|      _Alphen|    3|\n",
      "|oudersindekop|    3|\n",
      "|    jongejan7|    3|\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_df.groupBy('screen_name').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word count in DataFrames\n",
    "\n",
    "It is also possible to use DataFrames for less-structured data such as text. Here we show how you could do word count with DataFrames.\n",
    "\n",
    "The following chained query contains a number of methods you haven't seen before, and we'll go through it line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word| count|\n",
      "+----+------+\n",
      "|    |198753|\n",
      "| the| 23288|\n",
      "|   I| 22225|\n",
      "| and| 18653|\n",
      "|  to| 16373|\n",
      "|  of| 15725|\n",
      "|   a| 12796|\n",
      "| you| 12186|\n",
      "|  my| 10839|\n",
      "|  in| 10016|\n",
      "|   d|  8954|\n",
      "|  is|  8414|\n",
      "|that|  8343|\n",
      "| not|  8038|\n",
      "|  me|  7752|\n",
      "|   s|  7487|\n",
      "| And|  7457|\n",
      "|with|  6802|\n",
      "|  it|  6760|\n",
      "|  be|  6412|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "spark \\\n",
    "    .read.text('../data/shakespeare.txt') \\\n",
    "    .select(explode(split(\"value\", \"\\W+\")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To see what happens here, we break it down into steps. First we read in the data file and inspect the DataFrame. It contains one column, called `value` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|This is the 100th...|\n",
      "|is presented in c...|\n",
      "|Library of the Fu...|\n",
      "|often releases Et...|\n",
      "|                    |\n",
      "|         Shakespeare|\n",
      "|                    |\n",
      "|*This Etext has c...|\n",
      "|                    |\n",
      "|<<THIS ELECTRONIC...|\n",
      "|SHAKESPEARE IS CO...|\n",
      "|PROVIDED BY PROJE...|\n",
      "|WITH PERMISSION. ...|\n",
      "|DISTRIBUTED SO LO...|\n",
      "|PERSONAL USE ONLY...|\n",
      "|COMMERCIALLY.  PR...|\n",
      "|SERVICE THAT CHAR...|\n",
      "|                    |\n",
      "|*Project Gutenber...|\n",
      "|in the presentati...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swan_df = spark.read.text('../data/shakespeare.txt')\n",
    "swan_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The column name `value` explains why it is mentioned inside the `split` function. Let's call the `select` method but omit `explode` and see what happens. Notice, that with `alias` we rename the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                word|\n",
      "+--------------------+\n",
      "|[This, is, the, 1...|\n",
      "|[is, presented, i...|\n",
      "|[Library, of, the...|\n",
      "|[often, releases,...|\n",
      "|                  []|\n",
      "|       [Shakespeare]|\n",
      "|                  []|\n",
      "|[, This, Etext, h...|\n",
      "|                  []|\n",
      "|[, THIS, ELECTRON...|\n",
      "|[SHAKESPEARE, IS,...|\n",
      "|[PROVIDED, BY, PR...|\n",
      "|[WITH, PERMISSION...|\n",
      "|[DISTRIBUTED, SO,...|\n",
      "|[PERSONAL, USE, O...|\n",
      "|[COMMERCIALLY, PR...|\n",
      "|[SERVICE, THAT, C...|\n",
      "|                  []|\n",
      "|[, Project, Guten...|\n",
      "|[in, the, present...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df = swan_df.select(split(\"value\", \"\\W+\").alias(\"word\"))\n",
    "split_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the schema, we can see that `word` is actually an array of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Instead, we would like to have a row for each word, which is where [`explode`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) comes in. It has a similar meaning as `flatMap` in Spark RDDs. It gets rid of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       word|\n",
      "+-----------+\n",
      "|       This|\n",
      "|         is|\n",
      "|        the|\n",
      "|      100th|\n",
      "|      Etext|\n",
      "|       file|\n",
      "|  presented|\n",
      "|         by|\n",
      "|    Project|\n",
      "|  Gutenberg|\n",
      "|        and|\n",
      "|         is|\n",
      "|  presented|\n",
      "|         in|\n",
      "|cooperation|\n",
      "|       with|\n",
      "|      World|\n",
      "|    Library|\n",
      "|        Inc|\n",
      "|       from|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swan_df.select(explode(split(\"value\", \"\\W+\")).alias(\"word\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### User-defined functions\n",
    "\n",
    "In the previous example we used the built-in split function. It is also possible to define and use a custom user-defined function, or UDF. We'll show an example for the phone stock DataFrame first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+----------+-----------+\n",
      "|        model|  brand|stock|unit_price|       cost|\n",
      "+-------------+-------+-----+----------+-----------+\n",
      "|     iPhone 6|  Apple|    6|     549.0|  Expensive|\n",
      "|    iPhone 6s|  Apple|    5|     585.0|  Expensive|\n",
      "|     iPhone 7|  Apple|   11|     739.0|  Expensive|\n",
      "|        Pixel| Google|    8|     859.0|  Expensive|\n",
      "|     Pixel XL| Google|    2|     959.0|  Expensive|\n",
      "|    Galaxy S7|Samsung|   10|     539.0|  Expensive|\n",
      "|    Galaxy S6|Samsung|    5|     414.0|Inexpensive|\n",
      "|    Galaxy A5|Samsung|    7|     297.0|Inexpensive|\n",
      "|Galaxy Note 7|Samsung|    0|     841.0|  Expensive|\n",
      "+-------------+-------+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "exp_udf = udf(lambda price: \"Expensive\" if price >= 500 else \"Inexpensive\", StringType())\n",
    "\n",
    "phone_df.withColumn(\"cost\", exp_udf(phone_df['unit_price'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this manner, we can apply specialized function, like tokenizers, on DataFrames. However, we first must register them as UDFs and cannot simply define them inline with lambda functions like we can with RDDs.\n",
    "\n",
    "Below we define a very simple tokenizer, just as an example. It uses Python's string `split`, and also lowers the case of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def my_tokenize(s):\n",
    "    s = s.lower()\n",
    "    words = s.split()\n",
    "    return words\n",
    "\n",
    "returnType = ArrayType(StringType())\n",
    "\n",
    "tokenize_udf = udf(my_tokenize, returnType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 8\n",
    "Use the `my_tokenize` function from the last cell to count words on the Shakespeare DataFrame `swan_df` instead of usng the `split` function. Display the top 10 most occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|27549|\n",
      "| and|26037|\n",
      "|   i|19540|\n",
      "|  to|18700|\n",
      "|  of|18010|\n",
      "|   a|14383|\n",
      "|  my|12455|\n",
      "|  in|10671|\n",
      "| you|10630|\n",
      "|that|10487|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swan_df.select(explode(tokenize_udf(\"value\")).alias(\"word\")).groupBy(\"word\").count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
