\documentclass[a4paper]{IEEEtran}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\title{High Performance Computing and Big Data \\ Assignment 1}

\author{Federico Tavella, Student number 11343605}

\date{\today}

\begin{document}
\maketitle

\section{Cloud computing and Grid Computing 360-Degree Compared}

Lately, Cloud computing and Grid computing have been used as synonyms to indicate a technology that enable its users to obtain computational power without the cost of owning the physical devices. However, in Foster et al.~\cite{CloudGrid} the authors underlined how, despite the various shared characteristics (e.g. reduction of cost of computing and increment of reliability and flexibility) and purposes, Cloud and Grid computing are different under many other aspects. Here we summarise the most important points described in the paper. 

\subsection{Summary}

Firstly, Clouds and Grids differ for their \textbf{business model:} in a business model based on Cloud, a customer pays the provider based on the resources (e.g., gigabytes or computational power) he/she is consuming. On the other hand, a Grid-based business model is project oriented, in which the users - represented by a proposal (i.e., a project with the associated needed resources) - have a certain amount of service units that they can spend.

Secondly, they have different \textbf{architectures:} Grids focus on integrating existing resources with their HW, operative systems, local managements and security infrastructures. In fact, Grids have a special attention for interoperability and security issues since resources come from different domains. Cloud instead has been developed to address Internet-scale computing problem, so it is seen as pool of resources that can be accessed through standard protocols. Moreover, Clouds generally provide services at three different levels, namely \textit{IaaS}, \textit{SaaS} and \textit{PaaS}.
Despite these differences, there are some common flaws in \textbf{resource management:} in this case, both Clouds and Grids are facing challenges in the way the compute (e.g., scheduling, virtualization) and store (e.g., data locality, provenance, etc) data.

Foster et al. also provided some room for the two \textbf{programming model:} in Grid computing, the primary target is large-scale scientific computations and thus, users are highly interested in programming languages that can scale to leverage a large amount of resources. On the contrary, due to the lack of integrability of different services, in the Cloud world users take advantage of Web Services APIs.
Furthermore, there are some differences in their \textbf{application model:} Grids supports many kind of application, like high performance computing (HPC) and high throughtput computing (HTC), but Clouds are not expected to achieve the same efficiency as grinds in HPC because that would require fast and low latency interconnects for efficient scaling to many processors.

Finally, the authors discussed the \textbf{security models:} in Grids, there is the assumption that resources are heterogeneous and dynamic, and each site may have its own amministrator; thus, security is part of the design of the own Grid, addressing issues such as delegation, privacy, integrity, segregation, resource allocation, etc. On the contrary, Clouds have a simpler and less secure approach to security. For example, a new user can access any time to a Cloud resource providing some data, while for the Grids there are several stricter policies to enroll.

\subsection{Discussion}

Considering the seminars that have been held during the course, I think some of the points discussed in the paper need to be reconsidered. First of all, the distinction between Grids and Clouds is less sharp than 10 years ago: nowadays, Cloud is getting closer to the performance of Grids and it is also being used for different purposes. For example, there have been many more data intensive applications in the Cloud thanks to the emersion of Big Data. Moreover, some companies like SURFsara, are providing services using both Grids and Clouds. It is worth mentioning that most of the times Grids have better performance than Cloud in computing thanks to dedicated programming models, like OpenMP or MPI.
In addition to this expansion, Cloud is also starting to gain field in scientific projects: in many institutions, like universities, is becoming more and more common to use Cloud services to develop a piece of work. Researchers can estimate the resource consumption for their project and thus ask for foundings to deploy their project.

In my opinion, data locality is also one of the aspects in which Cloud is evolving: a lot of providers, like Amazon or IBM, are dividing their services and products based on regions. In this way, the machine in which the application is running is geographically close to the user, providing a better performance. It is worth to mention that in this case, Grid computing is obviously more efficient because most of the time, it has an ad-hoc network (e.g., part of the network is dedicated by an ISP to a specific entity), while Cloud computing takes advantage of the network used by everyone (i.e., Internet). A smart geographic positioning also improves the performance of remote data visualisation.

To sum up, in the last decade most of the progresses in the Cloud field improved its efficiency and efficacy to the point of getting closer to the Grid. As stated in~\cite{CloudGrid}, I believe it is very likely that in the future we are going to move towards a hybrid of the two different technologies, hopefully getting the best from both of them.

\section{The Pathologies of Big Data}

Big data is one of the buzzwords of the new century. Everyday, more and more data are produced and, consequently, they need to be stored. However, it seems that the amount of data increased a lot faster than the technologies to handle them. Thus, a conventional database (DB) or database management system (DBMS) are no more suitable for this job. In~\cite{BigData}, Adam Jacobs describes why this technologies are "old fashioned" in relation to big data and how they could be modified to improve their performance.

\subsection{Summary}

Data storage has been an issue for different decades. More than 30 years ago data were stored in tape libraries, with a really high access time. Luckily, nowadays thing are a little different: thanks to the improvement of physical storage devices (e.g., flash memories and SSD), we can store and retrieve data at an unbelievable rate. Truth to be told, most of the modern databases and database management systems are tought exactly to insert and retrieve data acquired in a transactional way. On the other hand, this is clashing with the main intent of big data, which is \textbf{analysis}. Data warehouses have been defined as copies of transaction data structured ad-hoc for query and analysis. This implies that these data need to be aquired from an operational database and reorganized for analytical purposes.
In fact, one of the main characteristic of big data is repeated observations during time and/or space. For example, scientific measurements are repeated from different spatial and temporal points to assess the validity of an experiment. Consequently, it may appears obvious that time order is really important for analyzing these data.

However, what makes modern relational database really unsuitable for big data is the fact that they explicitly ignore the ordering of rows in tables. In order to achieve good performance for analysis on such data, one must recognise the importance of sequentiality and reflect it at an implementational level (i.e., algorithms used to process this information). 
Over the past decades, computer science mainly focused on developing file systems that enabled \textbf{random access} at the expense of \textbf{sequential access} because it was thought to be more efficient. This is partly true, but it depends on the task. On the other hand, big data have one of their main trait in sequentiality, thus random access does not seem anymore the best solution. In fact, as stated by Jacobs, \textit{"on typical server hardware completely random memory access on a range much larger than cache size can be an order of magnitude or more slower than purely sequential access, but completely random disk access can be five orders of magnitude slower than sequential access"}.

Finally, there are two more topics that have been taken in account: \textbf{hard limits} and \textbf{distributed computing}. When designing and developing an application, everyone should keep in mind which are the physical limits imposed by the hardware. It is useless to produce an application to elaborate a huge amount of data if we can not fit them in our faster memory (e.g., RAM) and we are obligated to access to a slower memory (e.g., hard disk). One possible solution to this issue could be the distribution of the computation over different computers hardware, but this has inherently a significant performance cost, which can be compared to the one of accessing a disk. Consequently, data locality must be taken into account while distributing information over the nodes of a network. Nevertheless, we must also consider how the computation is distributed over the network, which is a non-trivial task and can affect the performance of the whole analysis.

\subsection{Discussion}

Data are leaking from every kind of device, from smartphones to IoT and wearables. More and more companies are interested into data analysis in order to make profit from them. To speed up this process, a lot has been done to parallelise the processing of these data (e.g., GPU, parallel and Cloud computing). However, a lot of crytical features were still missing.

Nowadays, there are some applications which are paying more attention on how to store and use data. For example, the \texttt{dmftar} file system developed by SURFsara used on Lisa cluster computer and Cartesius supercomputer is specifically thought for storing and transferring huge amount of data. There have been also some efforts in order to develop frameworks to integrate distributed and large scale data computing, such as Hadoop, Spark or Apache Flink. Moreover, there are some programming paradigms, like the functional one, that seems to be more suitable for this kind of application than the imperative or object-oriented ones; an example of a combination of functional and distributed systems-oriented language is Scala.

One possible solution to the latency introduced by distribution of data seems to be redundancy, but it obviously implies a lot of issues related to consistency and coherence between different nodes in the network.
It seems that the main focus in large scala data applications should be a proper design of how data are stored, with a lot of open challenges. Probably the best and simplest way to implement this idea is to use different database management systems for applications that want to make analysis on the stored data from the one that are simply quering them. In particular, it appears that NoSQL database are currently the most promising way of handling information for data intensive applications. In addition, they are also able to support SQL queries and that is why sometimes they are referred as "Not only SQL" instead of NoSQL.

In my opinion, as far as the current notion of big data is concerned, we made some progresses over the last decade. However, with the amount of digital information that will flood the web in the next years, a lot of challenges are still open and need to be addressed. Certainly, one of the main aspects will be focusing on the security aspects that are floating around big data.

\begin{thebibliography}{1}

\bibitem{CloudGrid}
Foster  et  al.  "Cloud  Computing  and  Grid  Computing  360-Degree  Compared," Grid Computing Environments Workshop, 2008. GCE '08, vol., no., pp.1,10, 12-16 Nov. 2008, doi:10.1109/GCE.2008.4738445

\bibitem{BigData}
Adam  Jacobs  “The  pathologies  of  big  data”,  Magazine Communications  of  the ACM ,Vol. 52 Issue 8, Aug. 2009. doi:10.1145/1536616.1536632

\end{thebibliography}

\end{document}